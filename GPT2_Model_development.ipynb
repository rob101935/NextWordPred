{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import re\n",
    "\n",
    "t = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "m = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in my: Next word: ['office', 'company', 'life', 'home', 'house']\n"
     ]
    }
   ],
   "source": [
    "starting_sentence = \"You are in my\"\n",
    "encoded_text = t(starting_sentence, return_tensors=\"pt\")\n",
    "\n",
    "#1. step to get the logits of the next token\n",
    "with torch.inference_mode():\n",
    "  outputs = m(**encoded_text)\n",
    "\n",
    "next_token_logits = outputs.logits[0, -1, :]\n",
    "\n",
    "topk_next_tokens= torch.topk(next_token_logits, 5)\n",
    "\n",
    "#putting it together\n",
    "print(f'{starting_sentence}: Next word: {[t.decode(idx)[1:] for idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data from LSTM testing set\n",
    "import pickle\n",
    "with open(\"data/test_nextWord.pkl\", \"rb\") as fp:  \n",
    "    actual_words = pickle.load(fp)\n",
    "with open(\"data/test_sentences.pkl\", \"rb\") as fp:  \n",
    "    previous_text = pickle.load(fp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "def predict_next_five(starting_sentence):\n",
    "    if starting_sentence in [\"\",\" \"]:\n",
    "        starting_sentence = \"  \"\n",
    "    encoded_text = t(starting_sentence[:-1], return_tensors=\"pt\")\n",
    "\n",
    "    #1. step to get the logits of the next token\n",
    "    with torch.inference_mode():\n",
    "        outputs = m(**encoded_text)\n",
    "\n",
    "    next_token_logits = outputs.logits[0, -1, :]\n",
    "    p = re.compile(\"[^a-z0-9']\")\n",
    "    # often returns punctuation for this version we only want words so take top 20 just in case and then filter down to have only words using regex\n",
    "    topk_next_tokens= torch.topk(next_token_logits, 20, sorted = True)\n",
    "    top_five_words = [p.sub('',t.decode(idx).lower()) for idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values) if len(p.sub('', t.decode(idx))) >0][:5]\n",
    "    # print(f'{starting_sentence}: Next word: {top_five_words}')\n",
    "    return top_five_words\n",
    "predicted_words = []\n",
    "for line in previous_text:\n",
    "    predicted_words.append(predict_next_five(line))\n",
    "# print(predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4375\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for i in range (len(actual_words)):\n",
    "    # print(f'{previous_text[i]}: {predicted_words[i]}: actual: {actual_words[i]}')\n",
    "    if actual_words[i] in predicted_words[i]: \n",
    "        j+=1\n",
    "        \n",
    "print(j/len(actual_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', '1', '2', 'is']\n",
      "['a', 'as', 'ing', 'i', 'r']\n",
      "['in', 'soon', 'to', 'with', 'on']\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_five(\" \"))\n",
    "print( predict_next_five(\"where\") )\n",
    "print( predict_next_five(\"I will be back \") )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test final package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'soon', 'to', 'with', 'on']\n",
      "['the', 'of', '1', '2', 'is']\n",
      "['the', 'and', 'is', 'of', 'a']\n"
     ]
    }
   ],
   "source": [
    "from next_word_pred import predict_gpt2 \n",
    "predictor = predict_gpt2.GPT2_next_word_pred() \n",
    "    \n",
    "print(predictor.predict(\"I will be back \"))\n",
    "print(predictor.predict(\"\"))\n",
    "print(predictor.predict(\"where\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CalabrioTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
